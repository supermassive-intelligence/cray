# Increase replicas for multi-node deployment
api:
  replicas: 2
vllm:
  replicas: 2

image:
  repository: tensorwave/scalarlm-amd
  tag: latest
  pullPolicy: Always

env:
  - name: HIP_VISIBLE_DEVICES
    value: "0"
  - name: ROCR_VISIBLE_DEVICES
    value: "0"

service:
  type: ClusterIP
  api_port: 8000
  vllm_port: 8001
  externalIP: 10.214.142.100 # Consider using load balancer or ingress for multi-node

jobs_pvc:
  storageClass: local-hostpath
  size: 100Gi
  accessMode: ReadWriteMany  # Changed to support multi-node access

cache_pvc:
  storageClass: local-hostpath
  size: 32Gi
  accessMode: ReadWriteMany  # Changed to support multi-node access

model: meta-llama/Llama-3.1-8B-Instruct
max_model_length: 4096
gpu_memory_utilization: 0.75

training_gpus: 1
inference_gpus: 1

max_train_time: 86400

# Add node affinity and anti-affinity to spread deployments
nodeSelector:
  gpu-type: amd
  gpu: "true"  # Based on the labels you added earlier
  kubernetes.io/os: linux

# Affinity configuration
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
            - key: gpu
              operator: In
              values:
                - "true"
            - key: kubernetes.io/os
              operator: In
              values:
                - linux
  
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app
                operator: In
                values:
                  - scalarlm
          topologyKey: kubernetes.io/hostname
